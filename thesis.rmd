---
title: "DRAFT Improving Forecasting for Foreign Policy ^[A summary of the forecasting tournament and its results are available on the [project website](https://corrod3.github.io/SecurityPolicyForecastingTournament/). Additional background information and methodlogical details can be access via the [online appendix](https://corrod3.github.io/SecurityPolicyForecastingTournament/appendix.html). The r scripts are available on the author's [Github account](https://github.com/Corrod3/SecurityPolicyForecastingTournament). For the raw data please contact the author.]"
subtitle: "Identifying drivers of forecasting accuracy in a forecasting tournament"
author: "Alexander Sacharow^[Hertie School of Governance, corresponding address: a.sacharow@mpp.hertie-school.org]"
header-includes:
    - \usepackage{fancyhdr}
    - \usepackage{float}
    - \pagestyle{fancy}
    - \fancyhead[LO,LE]{Improving Forecasting for Foreign Policy}
    - \fancyfoot[LO,LE]{Master thesis}
    - \fancyfoot[RE,RO]{Alexander Sacharow}
    - \usepackage{setspace}
    - \onehalfspacing
    - \newcommand*{\secref}[1]{Section~\ref{#1}}
date: "April 28, 2017"
abstract: In this paper, I explore the drivers of forecasting accuracy for geopolitical events with the help of a forecasting tournament. The paper analyses the responses of the participants in order to test and evaluate several explanations of successful forecasting. More specifically, it looks at (1) measurable characteristics of forecasters, (2) the decision environment in which a forecast is made and (3) minimal interventions aiming at improved forecasting judgements. My findings are that intelligence is [good/bad] indicator of forecasting success [reinforcing /contradicting] prior findings on geopolitical forecasting. I also find that moral judgment competency [is largely unrelated to / is a good predictor of] forecasting accuracy. Regarding the decision context, this research provides evidence for a [direct correlation between / decreasing marginal return on] time used for forecasting [and / in terms of] forecasting accuracy. Finally, the forecasting competition indicated that small interventions in form of analytical guides can [not] improve forecasting judgements. These insights are then used to aggregate crowd forecasts more efficiently. The results were derived from a security policy forecasting tournament which took place from February to April 2017 and which had more than 200 participants, comprised out of university students with a strong interest in the field, paid online respondents and voluntary online users.
output: 
  pdf_document:
    toc: true
    number_sections: true
fontfamily: mathpazo
fontsize: 12pt
urlcolor: blue
bibliography:
    -  literature.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Clear Global environment
rm(list=ls())

# Setting Working directory
try(setwd("D:/Eigene Datein/Dokumente/Uni/Hertie/Materials/Master thesis/SecurityPolicyForecastingTournament"), silent = TRUE)


source("main.R")

# Collect packages/libraries we need for paper:
packages <- c("stargazer")

# install packages if not installed before
for (p in packages) {
  if (p %in% installed.packages()[,1]) {
    require(p, character.only=T)
  }
  else {
    install.packages(p, repos="http://cran.rstudio.com", dependencies = TRUE)
    require(p, character.only=T)
  }
}
rm(p, packages)
```

# Introduction
<!-- 
Two possible structures:

1.	Abstract and short, but similar introduction
2.	 Abstract and long introduction outlining the big themes
a.	Structure 
i.	[What]
ii.	[The policy problem?]
1.	Reactive vs. forward looking policy
a.	Creating the future by studying it (Choucri, 1974, 64)
2.	 Working on wrong assumptions?
3.	Difficulty of probabilities
4.	 Ultimately decisions have to be made
iii.	 [The Question]
iv.	 [Why using forecasting tournaments]
1.	Forcing people to make calls
2.	In order to improve the current status-quo has to be known
v.	[structure]
3.	Alternative Intro Structure:
a.	Why forecasting is important
i.	Examples: 
b.	Famous failures
i.	 Irak example
c.	Question
i.	How to improve forecasting
d.	Other benefits of forecasting[?]
i.	Reactive/proactive


Why does it matter -->
Foreign policy makers, just like other decision makers, have to constantly think about the future and how it can unfold. They need to have an idea of possible outcomes and their likelihood in order to make their decisions. For this, they rely mainly on explicit or implicit forecasting judgements, either by themselves, their advisors or some outsiders. However, forecasting geopolitical trajectories in an uncertain world has proven to be a challenge. Too often forecasts are flawed and therefore unreliable for decision makers. In order to improve future-oriented foreign policy making, a better understanding of successful forecasting is crucial.

<!-- Motivation/ Why is it a policy problem -->
There are several ways how this can happen. First, a better understanding leads to better forecasts which are a prerequisite for pro-active policy. Foreign policy is often described as dominated by reactive policy making. In order to break this pattern, more reliable methods for forecasting possible futures are essential. Second, it will draw the attention to ill-conceived assumptions underlying today’s decision-making and thereby offers a chance to address them appropriately. Third, a better understanding of forecasting is necessary to transform institutions tasked with forecasting. Knowledge about individual differences between forecasters can be used to staff and structure such agencies. A better understanding of the decision environment will allow changing this environment to make it more suitable for forecasting. Likewise, tested decision aids can be used to counter common decision-making flaws made by forecasters. This research focuses primarily on the third point, but indirectly it does also contribute to the other points.

<!-- The basic question -->
In this paper, a forecasting tournament is used to identify drivers behind accurate forecasting. The tournament is used both to test prior findings in the literature and to deepen the knowledge on forecasting accuracy by testing new hypotheses. 

<!--justification forecasting tournament -->
Compared to other forecasting modes, a tournament has the advantage to force forecasters to make precise predictions and it evaluates these predictions based on their track record. This reduces the level of ambiguity in forecasts, which is common for many other forms of forecasting, and measures the quality of forecasts against what they actually attempt to do.

<!--limitations -->
However, it does also highlight the limitations of this research: A forecasting tournament defines successful forecasting in terms of accuracy. Participants have to specify the likelihood of a particular event in a given time frame in probability and they perform well if they choose probabilities close to the truth. Other indicators for successful forecasting are sidelined, e.g. identifying relevant possible future events or specifying the impact of certain future events.

<!-- Structure -->
This paper is structured as follows: First, the literature on forecasting and in particular forecasting competitions is reviewed and critically discussed. In the second section the hypotheses of this research and their theoretical as well as empirical background are presented. Third, the set-up of the research design is explained and some crucial design choices are reviewed. Fourth, the results of the research are presented. Fifth, the results are used to aggregate the individual forecasts. Finally, the research in general is discussed and some policy recommendations are drawn.


# Literature Critique
<!-- Possible vs. Luck -->
In order to forecast one has to make a basic assumption: Forecasting is possible at all. Not everyone, however, agrees to this. Forecasting sceptics emphasize the fundamental uncertainty of the future and assume successful predictions about world politics are ultimately grounded in luck [@Almond.1977; @Beyerchen.1992; @Taleb.2007]. They don’t claim that forecasting in general is impossible, but in their view the fundamental problems about foreseeing the future are particular strong in the field of international politics. And they have a point, as foreign policy has many  conditions which have proven to be unfavorable for forecasting: The environment is dynamic, most events are essentially unique, feedback on forecasts has long delays, there is a lack of empirical tested decision aids and a strong reliance on subjective judgments [@Shanteau.1992]. There are even indications that the quality of judgements gets worse with professionalization as intelligence experts specialized in forecasting seem to exhibit even more decision-making biases than college students [@Reyna.2014].^[Which has been attributed to bad habits developed in their working environment.]

<!-- Dispositional factors  --> 
But others scholars are more optimistic about the prospects of forecasting in the field. In their view, forecasting foreign policy is still in its infancy which is partially attributed to the dominant role explanation enjoyed in the past among international relation scholars [@Ward.2016]. This is, however, gradually changing as a result of more forecasting-oriented research. On the individual level, the research has shown grave differences between individual forecasters, making forecasting not only a question of how to forecast but also of who is forecasting [@Tetlock.2005; @BuenodeMesquita.2009; @Mellers.2015] and that forecasting can be further improved by appropriated training [@Mellers.2014]. There has also been a rise in the number of methods available and the data used to generate more sophisticated forecasts for world politics [@Dhami.2015; @Ward.2016].

This research is based on the presumption that forecasting is possible, but to a limited extent. Forecasting will never produce a fully certain prediction of the future and some events and aspects will remain beyond the forecastable. However, there are aspects which can be improved and the level of uncertainty can be reduced in a systematic and reliable manner.

<!-- 	Here maybe something on different time-horizons -->

<!-- [Choice of forecasting method]  --> 
The question is how to improve foreign policy forecasting in a meaningful way and what method to use for it. Generally, a wide variety of approaches is available: 
<!--	[intuitive predictions] -->
Starting from the simplest and most common one: Intuitive predictions. These are statements people make out of their head without recourse to a systematic methodology. The good thing about these predictions is that they can be applied to most topics at almost no cost. Unfortunately, they have a record of being inaccurate. Take for example affective predictions [@Wilson.2005] which rarely match the actual experience [@Schkade.1998] or probability judgments which have been shown to be susceptible to biases [@Kahneman.1974]. The most simple statistic models have shown to outperform intuitive predictions in various domains like university admission or parole violations [@Dawes.1989; @Swets.2000]. More recent research has demonstrated that expert prediction exhibit the same problems. In the political sphere, by tracking expert statements for more than 20 years @Tetlock.2005 has argued expert predictions are often as accurate as a “dart-throwing chimpanzee”. Similar results can be found with experts in other fields, e.g. climate science [@Green.2007]. One of the reasons for this discrepancy is that forecasting and explanation require different set of skills and there is no reason that experts combine both of them. Another reason is that experts tend not to apply the same rigidity to forecasting statements when asked about it than to their, often written, work. This does not mean expert opinions should be disregarded, but for forecasting purposes they should be treated with care.

<!--	Statistical models -->
One approach to solve the problems of inaccurate forecasting is to rely more on quantitative methods, as it is common in other disciplines like meteorology or economics. Statistical forecasting has been discussed for long in the sphere of international relations [@Choucri.1974] and the field is clearly on the rise [@Ward.2016]. Well known applications are election forecasting models [e.g. @LewisBeck.2005; @Norpoth.2010] or the work of the Political Instability Task Force [@Goldstone.2010], which led to the Integrated Crisis Early Warning System ([ICEWS](http://www.lockheedmartin.com/us/products/W-ICEWS/W-ICEWS_overview.html)) Project.  The forecasts are generally based on measurable input variables ranging from economic, media to political indicators. Using past data the models are calibrated and then used for extrapolating into the future.

<!-- 	[limitations]-->
But there are also severe limitations to this approach. First, statistical models are largely limited to quantifiable events which can be grouped by their similarity. Hence, possible forecasts are, for example, about the outbreak and scale of violence or protests. Many events in foreign policy, however, are at least to some degree unique. Take for instance a court ruling of the International Court of Justice on a specific matter. It is hard to impossible to build a statistical model for such types of events. Hence, there are often no statistical models available to forecast relevant events or important information has to be neglected in order to make events predictable with quantitative models. Second, many statistical models have focused on finding significant relationships instead of useful predictive indicators [@Ward.2010]. As a result, they do not produce precise predictions and therefore lack the external validity to be useful for actual forecasting. A wellknown example is the Flu forecast developed by Google which  could not produce accurate predictions after its initial introduction [@Lazer.2014]. Moreover, in many cases the necessary input data for quantitative models is scarce, not available or too expensive to gather. This limits the usefulness of quantitative models for policy makers. However, if there are statistical forecasting models with a good track record available, using them is surely a promising approach. The focus of this research is, however, more on geopolitical events for which quantitative models so far haven’t been able to produce useful forecasts.

<!--	[Prediction markets] -->
Prediction markets are another approach for generating forecasts which has been discussed on the literature [see @Wolfers.2004]. Economists see them as an efficient aggregator of information by encouraging market participants to use various information sources and by exploiting the wisdom of the crowd effect. This effect was famously described by [@Galton.1907], who observed that average of all estimates of the ox weight at an exhibition was much more precise than any individual estimate. Markets essentially attempt to incorporate this effect. A well-known prediction market example is the [Iowa Electronic Market](http://tippie.biz.uiowa.edu/iem/) , which has been used mostly to forecast elections in the U.S. But markets have been also used to forecast other political events as well, e.g. the outcome of referendums or even terrorism.^[There was a DARPA research project on Policy Analysis Market (PAM), it was however stopped after public criticism.]

For several reasons forecasting markets never really took off and still are of limited utility for policy making. One the hand this is due to the operating of prediction markets. One problem with market predictions is that they have shown to exhibit a favorite long shot bias. Small probabilities are overvalued and near certainty undervalued. This has been extensively discussed in the case of horse races, where people over proportionally bet on underdog horses [@Thaler.1988]. It was also shown to be a problem in financial markets [@Bates.1991; @Rubenstein.1994]. Other operational problems of prediction markets are trading by desires [e.g. @Forsythe.1999] and speculative bubbles. However, overall these problems tend to decrease with market size and sophistication.

More problematic for prediction market are its practical limitations: First, markets are not always feasible or even desirable. Take for instance the case of terrorism, where betting on these events create perverse incentives for conducting attacks. Or the case of asymmetric information, where strong insider knowledge exists and outsiders are basically discouraged to participate in the market. This is, for example, the case for many government decisions, where relevant forecasting information is only available to a small cycle of individuals. Second, prediction markets do often lack the necessary liquidity and number of market participants in order to produce meaningful predictions.^[Betting markets might be a bit of an exception, but so far betting has not been used much as a political forecasting tool for events other than elections and referendums.]

<!--	[forecasting tournaments] -->
A relatively new approach in forecasting international politics are forecasting competitions. In such a tournament participating individuals or teams are asked about the likelihood of future events. After the end of the forecasting horizon these forecasts are compared and evaluated. Forecasting competitions have some similarity to citizen election forecasts, where voters are asked which candidate or party they consider most likely to win their constituency [@Murr.2011]. Forecasting competitions combine the advantages of prior discussed approaches. Like intuitive predictions, they are hardly limited in their scope.^[The only serious restriction is, that the events under consideration are measureable in order to make subject to a forecasting competition.] They can incorporate reliable statistical models, if they are available. And like prediction markets, forecasting competition operate on the idea of aggregating the wisdom of the crowd and aggregating different information sources in order to generate forecasts.

In the field of international politics, this approach became prominent by the IARPA tournament, a geopolitical forecasting competition started in 2011 by the U.S. intelligence community. Different groups of academics were invited to participate in the project and compete against each other in providing forecasts about geopolitical events. In consecutive years, the winning team demonstrated that more accurate forecasts can be achieved by using two strategies: First, the extremization of individual judgements [@Baron.2014, @Satopaa.2015b]. Second, the exploitation of individual differences in forecasting skills and judgements [@Atanasov.2016, @Mellers.2015, @Satopaa.2017]. The first approach will not be discussed in detail here. Prior research on the second strategy has primarily looked at different predictors of forecasting success [@Mellers.2015; @Poore.2014]. This  paper is adding to this research by testing the replicability of prior findings and exploring new hypotheses.

<!-- advantage of competitions in comparison tp markets -->
The possibility of calibrating aggregated forecasts is a major advantage in comparison to prediction markets where market participants are usually anonymous and the individual leverage on the aggregated forecast is determined by the available capital of forecasters, which often comes from something else than their ability to forecast geopolitical events accurately. This is probably a main reasons why wisely aggregated forecasting competitions outperform markets [@Atanasov.2016].

<!-- [limits of forecasting tournaments] -->
Before I turn to the discussion of hypotheses, I want to touch upon the limitations of forecasting competitions. Limiting the understanding of successful forecasting to accuracy is their main weakness. One the one hand, forecasting is about more than just accurately describing the likelihood of events. It does also require exploring the unknown and identifying possible future events. In the forecasting competitions discussed here, this is not part of the tournament itself.^[Here this is done by the tournament facilitator, which is discussed in more detail in \secref{sec:tournament}.]  It ultimately requires recourse to other methods, e.g. expert opinion or strategic foresight [@Popper.2009; @Kosow.2008; @Bergheim.2009]. On the other hand, accuracy might not be the most relevant quality of forecasts for policy decision making. For this other dimension like the impact or the possibility of early action might be more important. 

There are, however, several reasons why accuracy should be considered of key importance for forecasting. First, the likelihood of future event is essential for decision making from a normative point of view. Decision theory under uncertainty, and in particular its most used approaches (expected utility theory [@Neumann.1944] and subjective expected utility [@Savage.1972]) presuppose the decision maker has an idea of the likelihood of the outcomes. Moreover, in practice decision makers tend to ask for a likelihood assessment when presented with possible future scenarios.^[Observation made by scenario and foresight experts which they expressed to me in a private conversation.]  Second, improving accuracy is a process which has positive effects on the other aspects of forecasting [@Tetlock.2015]. It creates good incentives for better forecasts in a broader sense by reducing ambiguity in forecasts, encouraging learning from mistakes and forcing the organizers of a forecasting competition to explore the “unknown”. Third, more accuracy in foreign policy is possible and can be consistently utilized by analysts, e.g. by aggregating more information into single forecasts or discussing the implication of single pieces of information [@Friedman.2016].

# Theory and Hypothesis

<!-- [Intro / Justification] -->
Forecasting competitions can be used to improve forecasts by identifying good forecasters, the environment in which they make good decisions and decision aids which can improve the quality of the judgement. Following this idea, the factors discussed in the paper can be divided along three categories: dispositional, environment-related and intervention. Dispositional factors are characteristics of the forecaster like abilities. They don’t have to be fixed, but they should be at least stable on the short-term and measurable by a third party. Environment-related factors refer to the situation in which the decision is made. Intervention refers to treatments by outsiders, which aim at improving the quality of a decision.

<!-- [Dispositional factors] -->
Accepting the idea of individual differences in forecasting skills implies that there are individual dispositions which have direct implications on forecasting. In order to identify good forecasters it is therefore essential to know what characteristics can predict successful forecasting.

<!-- [intelligence and why] -->
A good starting point for this is intelligence. It has been shown to be a good predictor for many other things like job performance [@Ree.1992; @Schmidt.2004], socio-economic status [@Strenze.2007], academic achievement [@Furnham.2009] and decision competence [@DelMissier.2012; @Parker.2005]. Therefore, it can also be assumed to be a valuable predictor for forecasting success. 

<!-- [what is intelligence] -->
Intelligence can be defined in different ways. Generally, it is conceptualized either one-dimensional or multi-dimensional. In the one-dimensional approach a meaningful intelligence measure can be collapsed into one variable, while in the multi-dimensional approach intelligence is understood as a collection of different abilities. 

<!-- [prior findings on intelligence / forecasting] -->
@Mellers.2015 have argued that there are three relevant aspects of intelligence for geopolitical forecasting. They are (1) inductive reasoning (e.g. linking current problem and historical analogy), (2) cognitive control (e.g. override seemingly obvious but incorrect responses and engage in more prolonged and deeper thought) and (3) numeric reasoning (understanding mathematical dimension of a problem). In their research the different aspects were correlated to more accurate forecasts and they are correlated between each other. The first finding shows that intelligence measures can be used for predicting forecasting success and the second point supports the view that a one-dimensional intelligence concept is sufficient in this context. Similar results were derived by @Poore.2014, who found a strong correlation between various measures for analytical abilities and forecasting accuracy. Therefore it is reasonable assume these findings will be confirmed by the security policy forecasting tournament:

**Hypothesis 1a: More intelligent individuals are more accurate forecasters**

<!-- [how is it measured] -->
To test the hypothesis, it needs to be clarified how intelligence is measured. Various psychometric measures are available. @Mellers.2015 for example use three different methods: The Ravens Advanced Regressive Matrices method [@Bors.1998], the Cognitive Reflection Test (CRT) by [@Frederick.2005] and a combined numeracy scale from @Lipkus.2001 and @Peters.2006. @Poore.2014 used self-reportet SAT scores, sample GRE/SAT questions, subjective numeracy [@Fagerlin.2007] and, like @Mellers.2015, the CRT. 

<!-- [test used in this paper] -->
This paper uses a test which was not used for forecasting accuracy yet: The Berlin Numeracy Test (BNT) by @Cokely.2012. It is a relatively new psychometric scale that was in particular developed to access statistical numeracy and risk literacy. This makes the test in particular suitable for forecasting decisions involving probablity judgements. Since it is especially suited to differentiate between individuals with higher education, it is also likely to perform better than the numeracy scale used by @Mellers.2015 which lacked differentiation. Another reason for the BNT is its length: The test consists only of four questions, making it suitable for one time forecasting tournament. 

<!-- [Moral Judgement] [transition to second dispositional factor]-->
Intelligence is a common measure, but by far not the only dispositional factor of interest. Other factors considered in this context include personality and cognitive styles [@Mellers.2015; @Poore.2014]. It has been found that factors like openness to new experiences and active open-mindedness are good predictors for forecasting success. To keep the forecasting tournament within reasonable time limits for the participants, this reaseach did not attempt to replicate these results. Instead, the paper scrutinzes a factor which was not tested before, but was raised by @Tetlock.2015 [p. 226] in a side note: The interference of moral judgements with analytical judgments. In his book Tetlock claims  superforecasters, unlike other forecasters, can separate analytical judgements from moral judgements. This is in particular important when forecasters make judgements about events for which they have a strong moral position. In such cases many people mingle the desirability of an outcome with the likelihood assessment. For example, if a person holds a strong moral opinion on the Syrian government and has a strong desire for it to be replaced by a more human-rights oriented government, Tetlock assumes this person to skew his or her probability judgment on the fall of the government towards the desired outcome.

This idea is similar to the social desirability bias where individuals over-report characteristics about themselves which they consider socially desirable [@Dalton.2011] as well as to trading according to desires in prediction markets, where personal political preferences affect buying decisions [@Forsythe.1999]. Analogously, people are expected by Tetlock to overrate the likelihood of events they deem desirable.

<!-- Strategies to test this: 1. Asking respondents about probability and desirability -->
But is this true? There are several ways this could be tested. First, the straight forward approach would be to ask respondents not only about the probability of an event but also about the desirability of it. The information could then be used to see whether it has any relationship to forecasting accuracy. But this approach does not help us further here. One the one hand the desirability of the events in question is unknown, although, this could theoretically have been asked in the forecasting tournament. One the other hand, for practical and normative reasons the accuracy of a forecaster should be predictable without knowing her or his moral opinion on the subject. Practically, it would be unfeasible as one would always have to ask forecasters about probability and desirability of events they forecast. This would increase the workload and induce fatigue. Normatively, it is undesirable to force forecasters to reveal their personal opinions as many of them are likely to work in highly politicized environments.

<!-- 2. Ranking questions by their morality -->
Another way would be to rank the questions according to their morality and see whether forecasters accuracy is related to the morality level of a question. However, I am not aware of any morality scale which could be applied to forecasting questions and which is sufficiently universal. To the contrary, it is plausible to have people disagree about the morality of geopolitical events. For example, many in the West would like Assad to lose power of the Syrian government while a Syrian Alawite likely will have a very different view on this. Moreover, assigning morality levels to questions introduces a measurement problem: It cannot be distinguished whether the source of the inaccuracy steems from the uncertainty of an event in question or its morality.
    
<!--3. Psychometric measure: -->
Hence, we might have to rely on a less direct test of the relationship of moral and analytical judgement. The (psychological) theory of moral judgement might be a promising starting point for this. According to @Kohlberg.1958  the moral development of humans can be ordered on a scale. This scale reflects how sophisticated moral justifications of judgements are. In this context, the concept of moral competency was developed. It describes how people can disentangle moral decisions along this scale and see consistently the differences between them. This has some similarity of that we are interested in the context of forecasting questions. If moral competent individuals are better able to differentiate between different moral justifications, we might also assume that they can better differentiate their moral and their analytical judgement in the context of a forecasting question. In order to test this, the following hypothesis is used:

**Hypotheses 1b: More moral competent individuals are more accurate forecasters**

<!-- [moral-judgment competence] -->
More precisely, moral competency can be definded as “the ability of a subject to accept or reject arguments on a particular moral issue consistently in regard to their moral quality even though they oppose the subject's stance on that issue” [@Lind.2008, p. 200]. It can be contrasted with opinionated judgements, which are intuitive and emotional reactions to the content at hand and much of what Tetlock understands by moral assessment. Moral competency hence captures the idea that individuals are willing to consider counterviews despite their own, potentially strong, view on the issue.  
In this regard moral competency has similarities to the active open-mindedness measure [@Baron.2007], where respondents are asked whether they would consider other opinions and which has been shown to be a good predictor of forecasting success [@Mellers.2015]. But it goes a step further: Instead of relying on a self-assessment, it actually tests whether different arguments are considered in the face of a moral issue. The hypothesis is therefore:

<!-- how to measure it -->
Moral competency can be measured with the moral competency test (MCT). The test confronts respondents with two moral dilemma situations. To each story the participants have to answer 12 questions on different justification for the described acts. The answers are used to compute a competency score for each respondent. The score is not based on correct and wrong answers, but reflects a ratio between different parts of the answers.
\footnote{Basically, a multivariate analysis of variance (MANOVA) is used here which measures how individuals disaggregate different moral stages. Based on the instructions of the test (Lind, 2008) I reengineered the underlying formula for the score: \begin{equation} c = \frac{\frac{1}{4}\sum^{6}_{i = 1}(\sum^{4}_{j=1}x_{ij})^2 - (\frac{1}{24}\sum^{6}_{i=6}\sum^{4}_{j=1}x_{ij})^2}{\sum^{6}_{i = 1}\sum^{4}_{j=1}x_{ij}^2 - (\frac{1}{24}\sum^{6}_{i=6}\sum^{4}_{j=1}x_{ij})^2}\cdot 100 \end{equation} where $i \in {1,...,6}$ stands for the stage and $j \in {1,...,4}$. for the section (pro-Worker, contra Worker, pro-Doctor, contra-Doctor). The score is between 0 and 100 and is sometimes categorized as follows: very low (1-9), low (10-19), medium (20-29), high (30-39), very high (40-49) and extraordinary high (above 50) (Lind, 2008, p. 200). For further information check the \href{https://www.uni-konstanz.de/ag-moral/mut/mjt-engl.htm}{associated website}.}

<!-- [Changing environment of decision making] -->
Individual differences in forecasting accuracy are also determined by differences in the decision environment. Unlike early decision theory assumed, empirical research has shown that context matters [e.g. @Kahneman.1974]. The list of possible factors is long, ranging from the number of forecasters, the incentives to the opportunity of deliberative practice [@Arkes.2001; @Ericsson.1993; @Kahneman.2009]. In this paper only the simplest environmental factor will be tested: Time used for answering the forecasting questions.

<!-- [Time] -->
Time used for forecasting is ultimately a choice of the forecaster. It can, however, be influenced by explicitly making time slots available or freeing forecasters from other tasks. To justify such choices, it would be necessary to know the relationship between time and forecasting accuracy. There are good reasons to belief that more time will also lead to more accurate forecasts. First of all, time is necessary in order to go beyond intuitive thinking and to engage in analytical thinking about the question at hand. The two different ways of thinking are often described as ‘system 1’ and ‘system 2’, where system 1 stands for fast intuitive judgements and system 2 for slow reflective thinking [@Evans.2013; @Kahneman.2013]. There is also a second reason why more time might lead to more accurate forecasts: Spending time on a question does allow gathering more information. Hence, the use of time should indicate whether a decision was informed or not.
 
However, there are two reasons why this might not be the case. First, the forecasters might use the time for other things. @Haran.2013, for example, have argued that acquiring new information depends on other characteristics like active open mindedness. But this is unlikely to be a problem in this forecasting tournament as participants would have no reason to spend more time on the questions and move on to the next section of the tournament. Second, the forecasters might have different levels of pre-knowledge. Hence, some participants might need more time to grasp the context of questions while others can rely on their extensive political pre-knowledge. As will be later described in more detail, this was counteracted by ensuring a wide span of questions. Moreover, going back to the first explanation for the link between time and accuracy: Even individuals with pre-knowledge will have to switch between the two mental modes, which should again be captured by the time spend on the questions.
<!-- add: Forecasters might be overwelhmed by too much information (Armstrong 1985, 100-102.)  from Armstrong 2004 FAW forecasting-->

But the relationship between time and accuracy is unlikely linear. From the view of a mental system shift, there is no theoretical reason why more time should increase accuracy once the shift of mental systems took place. From the informational point of view, over time the value of new information decreases as it will have less and less implications for the judgement and the costs of gathering will increase as it will be harder to find new additional information. For this reason, it is reasonable to assume that the marginal value of more time decreases: 

**Hypothesis 2: The marginal added value of time spend on forecasting is positive and decreases over time**

In order to verify the hypothesis, the time participants used to answer the forecasting questions is measured. The validity of the measurement is ensured by treating other questions asked over the course of the forecasting completion in different sections and thereby excluded from the time measurement for this hypothesis. Moreover, the time used for forecasting is cross-checked with self-reported time use.

<!-- [intervention] Discussion: Treatment effects on judgement in general -->
Finally, differences between individual forecasters can be the result of outside interventions. In forecasting this could be achieved by treatments improving analytical judgements [@Soll.2015; @Larrick.2004]. Possible interventions include minor decision aids [@Kretz.2015], feedback [@Benson.1992], exposure to multiple perspectives [@Ariely.2000; @Herzog.2009], exposure to historical analogies [@Lovallo.2012], decomposition of problems into subsets [@Fischhoff.1978], explicit consideration of contradictory evidence [@Koriat.1980] and probabilistic training [@Mellers.2014].  They can be tested in a forecasting tournament. @Mellers.2014, for example, tested the effect of scenario and probabilistic trainings and found them to have a long-term positive effect on forecasting accuracy.

However, for the forecasting tournament in this paper the focus will be on minimal interventions as the scale of the tournament is rather small. It is therefore more suited to test mild interventions aiming at debiasing. This could, for example, be achieved by providing forecasters with analytical tools. @Kretz.2015 has done some research on mild decision aid interventions and found that most analysts tend to disregard decision aids which require some effort, at least after some time. Kretz sees the reason for this in the additional mental capacities needed to apply the decision aids, which distracts the analysts from the actual problem at hand. In his research only the mildest intervention proved to have a significant effect on the judgement quality. 
<!-- Following this idea, the paper tests a minimal intervention in the context of forecasting and tests the following: -->

Following this idea, I decided to use a decision guide as treatment. The guide is inspired by Tetlock's description of how "superforecasters" approach forecasting question [@Mellers.2014]. Theoretically speaking, it advises the forecaster to find a reference class for the forecasting question and then use Bayesian updating to adjust the base probability with other information.^[The reference class raises fundament issues about interpretating probabilities, as for single events the classical frequency view of probabiliy does not work [e.g. @Popper.1959]. Hence, the forecasters have to pick a reference class based on some similarity criteria.] Practically, this means forecasters were advised to identify a base rate for event in the forecasting question (outside view) and add or substract incrementally probability points from this depending on the nature of the available information (inside view). Basically, this is a decision heuristic. The decision guide might improve forecasting accuracy for two reasons: First, the decision heuristic is based on standard theory for decision under uncertainty. This theory reflects the view most decision theorists have on how such questions should be addressed. Second, the decision heuristic of out- and inside view has been empirically successful for other types of forecasting decisions, e.g. for company revenues [@Lovallo.2012]. 

**Hypothesis 3: A decision guide increases the accuracy of forecasting**

In order to test this, all participants are assigned randomly, in about equal shares, to a treatment or control group. The treatment group is provided with a short (ca. 150 words) decision guide which describes the idea of outside and insight view in simple words and illustrates it with an example.^[The full text of the guide is available in the [online appendix](https://corrod3.github.io/SecurityPolicyForecastingTournament/appendix.html)]  At the end of the guide, they were asked to fill out a small check box on whether they have read the guide.

# The Forecasting Tournament {#sec:tournament}

<!-- [When, where, Who] -->
The forecasting tournament took place from 06.-12. February 2017 and forecasters were asked to consider possible events happening between 12th of February and the 24th of April 2017. The forecasters came either from the Master of International Relations program at Hertie School of Governance Berlin or were recruited externally via mailing lists of relevant study programs, associations working in the field of international relations or by word of mouth. The students had to do the tournament as a homework while the others participated voluntary. The group was further complemented by forecasters from Amazon Mechanical Turk, who were paid for their participation. In total, 214 forecasters provided valid answers, they had an average age of `r round(mean(SPFT$age),1)` and `r round(length(SPFT$sex[SPFT$sex == "Female"])/length(SPFT$sex),3)*100` percent of them were female.  

<!-- [Design/how] -->
The forecasting tournament was conducted with a one off online survey. The survey consisted of three parts: First, the forecasters were asked question batteries of psychometric measures on intelligence and moral judgement. In the second part, the participants answered 24 forecasting questions on various security policy related events.^[A full list is the questions is available in the [online appendix](https://corrod3.github.io/SecurityPolicyForecastingTournament/appendix.html)] In each forecasting question the participants were asked to provide a judgement of how likely they thought the event is, expressed in probability. Finally, in the third part, forecasters reflected upon their forecasting and provided some demographic information about them.^[The survey was implemented with Qualtrics.]

The forecasters were informed that their background information will be handled confidentially and in case of the university group that their forecasting judgments will be accessible to their fellow students. The second notice had the intention to incentive the students to give serious consideration to their answers by creating a competitive environment. In case of the voluntary participants this was less important as their participation already indicated intrinsic motivation. For the Mechanical Turk users the survey used attention checks, which they had to pass in order to receive the payout. All participants were also explicitly informed to use all information sources the deem relevant and spend as much time on the questions as they need. The participants were not informed whether to work individually or in teams as this lies outside of the control of the research design, but `r round(100*length(SPFT$team[SPFT$team != "Individually"])/length(SPFT$team), 1)` percent said they answered the questions not alone. In total 231 individuals participated in the security policy forecasting tournament. However, only 214 responses are used as some participants had signs of not taking the survey serious (failing attention checks, unrealistically short time used for participation) and or submitted their forecasts after February 12th, 2017.

<!-- [Questions] -->
The questions were all related to security policy and selected in a multi-stage procedure. In the first step, I selected conflict regions which might be subject to changes in the short time horizon under considerations. Then I drafted questions by surveying reports from international organizations, think tanks, governments, NGOs and media outlets on recent developments in the conflicts. Among these sources were reports by the International Crisis Group, the German Institute for International and Security Affairs (SWP Berlin), German Institute of Global and Area Studies (GIGA), Brookings Institute and the Carnegie Center. Finally, the draft questions were sent to a few researchers and forecasting experts for feedback and their recommendations were integrated in the final forecasting tournament.

The questions were all binary and the possible answers “yes” or “no”. Most questions covered possible events in the whole time period between February 12th and April 24th, 2017. For each question the forecasters had to specify a probability to indicate how likely they expected the event to be. One question was, for example, “Will IS claim responsibility for another attack with a truck inside the European Union by 24. April 2017?”. The questions have to be precise and measureable. How difficult this is, one can see by the mentioned question. On March 22th, 2017 the Westminster Attack happened in the UK. The attacker used a SUV to attack and kill several people in London. The question was intended to capture such events, but the term ‘truck’, literally understood, does not include SUVs. This is a fundamental problem about outlining events which did not happen yet: There will be aspects which were not anticipated correctly. In this case, the type of the car. How to respond in such cases? Here the event was nevertheless seen as a ‘yes’ reply to the question. First, the question was intended to capture such events and the use of an SUV instead of a truck does not make it fundamentally different. Second, suppose one would ask the participants whether their expectation explicitly excluded the case of SUV being used the likely answer would be no. However, there is no fixed rule for these borderline cases and they need to be decided on case to case basis.

Selecting the questions illustrates the distinction between two challenges in forecasting: Sampling and accuracy. In this research design sampling is done by the organizer of the forecasting competition while participants are solely dealing with the issue of accuracy. Ideally, one would also include sampling into a competition and testable format, but samples of different possible future events are hard to compare and therefore they cannot easily be made part of a competition.

<!-- 1. criteria question: neither remote nor almost certain-->
As a good sample of possible geopolitical events is crucial for the forecasting tournament, the forecasting questions had to satisfy a number of criteria. First, the questions should neither concern events which have almost no chance of happening nor almost certain events. It is difficult to select a highly unlikely event as they are numerous and can have all kinds of realizations. An example for such an event is the start of the Arab Uprising in 2011 after Mohamed Bouzid set himself on fire. Neither should almost certain events be subject to a forecasting competition. An example for such a question would be whether the German federal elections will take place in September 2017. To some degree such a question is a just the flipside of the highly unlikely event, but without specifying what this interruptive event could be. But again, choosing a relevant almost certain event for a forecasting competition will become an arbitrary choice. Moreover, remote and almost certain events will cause clustering of forecasts along the extreme values by participants in the forecasting competition. This would make it harder to distinguish successful forecasters from unsuccessful ones.

<!-- 2. criteria: different regions -->
Second, the questions should cover various regions. The results might be biased if individual participants have special knowledge about a region which is overly represented in the competition. To further reduce the effects of narrow expertise, similar questions were also avoided. Ideally, the questions should also cover a wide range of policy fields. However, as the security policy forecasting tournament was conducted in collaboration with a university course at Hertie School of Governance, the topics were restricted to content of this course. This should not be a problem, as the prior mentioned considerations already introduce diversity into the questions and even within the field of security policy there is a wide range of possible topics.

<!-- 3. Relevance for policy making -->
Third, events for the forecasting competition should be relevant for policy makers and a large group of people. Relevance implies that the event has an impact on policy makers. The impact dimension excludes forecasting questions like the music played at the inauguration of a head of state. Relevance in this research was ensured by selecting events or indicators which would be discussed or considered by international organizations, governments and policy-oriented research institutions.

<!-- Limits on choosing the questions -->
Even though these criteria guided the selection of the questions, a few limitations had to be taken into account. First of all, language restricted the range of possible events. Only events which would be reported in English language were selected for the competition. It reduces the problem of forecasters benefiting from the knowledge of certain languages. This could, for example, be the case with Spanish as some events in Latin America most information would be in Spanish. However, the more significant events are the more likely there is also sufficient information in English available. Second, the events were chosen on the basis of possibly getting international media attention. On the one side, this reduces the barrier for participants as it limited the scope of the questions to topics they might at least generally familiar with. On the other side, it keeps the workload for tracking questions reasonable. However, this does also exclude many possible questions. For example, funding decisions in international organizations are of policy relevance and might have severe implications, but information on them are hardly available. Third, for the events should be reliable information available. In the field of security policy this can be difficult as information are inherently subject to the conflict dynamics and in many conflict areas almost any reliable information is hard to get by. Take for instance the conflict in the Democratic Republic of Congo or even Syria, where smaller incidences are rarely reported, and even if, cannot independently be confirmed.

<!-- Brier score: General -->
In order to assess the quality of forecasts, they have to be scored. A common method is based on the Brier score, which was originally proposed in the context of weather forecasting [@Brier.1950]. Generally speaking, the Brier score indicates the distance of the forecast to the truth. More precisely, the Brier score is the squared error of a probabilistic forecast. To calculate it, the forecast are expressed on the range between 0 (0%) and 1 (100%). The realized events are coded either 0 (if the event did not happen) or 1 (if the event did happen). For each answer option, the difference between the forecast and the correct answer is squared and added. It can be expressed with: 

\begin{equation} 
  \frac{1}{N} \sum^{N}_{i=1}\sum^{R}_{k=1}(p_{ik}-o_{ik})^2
\end{equation}

<!-- Brier Score: Details + Scoring Board rules-->
$N$ stands for the number of events, $R$ is the number of possible classes the event can fall, $p$ is the probability forecast and $o$ the realized outcome. The Brier score can evaluate questions with more than two possible outcomes ($R>2$), but in this paper only binary events are considered ($R=2$).^[As the competition only includes binary events, a simpler version of the Brier score (which is equivalent to the squared error) would also be sufficient. But to make comparison to the Good Judgement easier the multinomial version of the Brier score is used here. The simple Brier score can easily be computed by dividing the multinomial Brier score by two.] The best (lowest) possible Brier score is 0, and the worst (highest) possible Brier score is 2. The Brier score is a proper scoring function which means that participant cannot improve their score by reporting a different probability from their actual belief. The participants' Brier score are computed by averaging the Brier scores across the questions.^[In contrast to @Mellers.2015 the scores don't have to be normalized as the participants had to answer all questions and could not self-select the questions they thought to be easiest.]

<!--Descriptive statistics -->

# Results

<!-- [Brier score distribution] -->
The research design aims at identifying different factors of individual forecasting success. For this, measurable differences between the individuals are a prerequisite. In the case of a forecasting tournament, this can be verified by looking at the distribution of Brier scores (Figure \ref{fig:brier}). Since the distribution ranges from `r round(min(SPFT$brier.avg),2)` to `r round(max(SPFT$brier.avg),2)` with a mean score of `r round(mean(SPFT$brier.avg), 2)` we have enough variance for further testing. 

```{r echo = FALSE, fig.cap="\\label{fig:brier} Brier score distribution", out.width=c('300px', '140px'), fig.align='center', fig.pos = 'H'}
brier.plot
```
<!-- Skill vs. Luck  / Pre-Hypotheses /  T Test -->
Before turning to the hypotheses, it makes sense to see how the participants performed in comparison to a simple statistical benchmark. This gives us a picture of the overall forecasting ability of the participants. A standard benchmark is the comparison of the realized forecasting scores to a situation with uniformly random distributed  outcomes [@Mellers.2015]. The average Brier score with random events is `r round(mean(brier.exp.fq),digits=2)`, while the actual average Brier score of the participants was `r round(mean(SPFT$brier.avg),digits=2)` (`r t.test.against.random`).^[In this context a one-sided t-test is used to see whether the forecasters performed better: $H_0: \bar b =b_rand$ and $H_A: \bar b  <b_{rand}$. The Brier score for random events was computed by computing the expected Brier score for each question / individual and taking the average: $\frac{1}{24 \cdot 214} \sum^{24}_{q = 1} \sum^{214}_{i = 1} (p_{qi}^2 + (1-p_{qi})^2)$ with probability forecast $p \in$ [0,1], individuals $i \in \{1,..., 214 \}$ and questions $q \in \{1, ...,24\}$.]
<!--	[evaluation results] --> 
Hence, the forecasting crowd performed significantly better than the benchmark. This supports the view of forecasting optimists: To some degree forecasting seems possible.
<!-- ^[However, if we compare the performance of the crowd to a simple 50% guess for each question (brier score = 0.5) the forecasting crowd on average did not perform better.] CHECK-->

<!-- The alternative *b. The result shows that on average forecasters did not perform better than a random quess[, they even performed worse]. This is not a surprise as it is in line with the literature on intuitive and expert judgements. Rather than looking at the group average we should look at successful forecasters and understand why they outperform the group average.* -->

<!--[Alternative measure: Proportions of forecasts on the correct side of 50%] -->
A second and more intuitive measurement of the overall forecasting accuracy of the participants is the proportion of questions where  forecasters with their forecasts were on the correct side of 50%  [@Mellers.2015, p. 6]. The measure counts the forecasts above 50% for event which happened and forecasts below 50% for events which did not happen and divides them by the total number of forecasts. The perfect score would be 100%, a score corresponding to chance 50%. The forecasting average of the tournament participants was `r round(mean(SB.CS$cs.avg)*100, 1)`%. Again, we can use the t-test to measure whether the difference to the chance score is significant (`r t.test.correct.side`).

<!-- [evaluation] --> 
Like for the first benchmark, this measurement indicates that forecasters performed better than random guessing. However, it does also illustrate that on average the forecasters are just slightly better than chance. For comparison: In the Good Judgement Project [@Mellers.2015, p. 6] forecasting competition the share was 75%, indicating that their forecasters crowd performed better.^[A third measure for the overall performance of the crowd would be to compare the average score of the crowd to the performance of 50% guess for each question, which would basically assume the decision maker is ignorant to any information. A 50% guess for each question is equivalent to a Brier score of 0.5. Hence, according to this measurement the crowd actually performed worse than a simple 50% guessing strategy and the crowd's performance looks less favourable compared to the other measures. But this is not a problem, as the primary focus here is to understand the individual differences between forecasters and why some managed to outperform the rest.]  <!-- check all of this after 24.04.-->

<!-- The correlation between brier scores and the correct side measurement could be computed in order to show that both measures capture accuracy
Alternative: *b.	Like the first benchmark, the alternative measure shows that forecasters perform on average [like / worse] than random guessing.* Since the measure tries to capture the same thing, this is not surprising. --> 

\begin{table}
\label{tab:desc}
\centering

```{r results = "asis", echo = FALSE}

stargazer(select(SPFT, brier.avg, bnt.s, mct.c, time.fq.sec, Duration.min, age),
          title = "Descriptive Statistics",
          covariate.labels = c("Brier score" ,"BNT Score", "MCT Score", 
                               "Forecasting time in min", "Total time in min", 
                               "Age"), 
          header = FALSE, float=F, digits = 2)
```
\caption[desc]{Descriptive Statistics}
\end{table}

<!--	[Desciptives for Hypothesis 1a] -->
Having established the performance of the crowd, the focus can now turn to factors behind the forecasting success of individual forecasters. Starting with the first dispositional factor: intelligence. The forecasters mean score at the Berlin Numeracy Test (BNT) score was `r round(mean(SPFT$bnt.s),2) ` (at a range from 0 to 4), which is slightly above the 1.6 average score @Cokely.2012 found for Berlin university students.^[The forecasting tournament used the four item 'paper and pencil' version of the test. For this version there is no general population score for comparison available. However, in principle it would be possible by adjusting the score to the to the more commonly used adaptive test format. For further details see @Cokely.2012.] As the distribution of BNT scores illustrates (Figure \ref{fig:bnt}), the test is able to discriminate between the participants. It assigned the forecasters to five score levels, each of which is roughly equal in size.

```{r echo = FALSE, fig.cap="\\label{fig:bnt} Berlin Numeracy Test score distribution", out.width=c('240px', '150px'), fig.align='center', fig.pos = 'H'}
bnt.plot
```

<!-- evaluation of hypothesis 1a-->
In order to understand the relationship between intelligence and forecasting accuracy, the Pearson correlation coefficient is informative. It computes the direction of the relation, indicated by the sign (+ or -), and the magnitute of the effect on a range from 0 to 1. The correlation between the BNT score and the Brier score is: `r cor.brier.bnt`. With the t-test it can be further assessed whether the result is significantly different from no relationship between both variables ($r = 0$).
\footnote{Here the following t-test statistic is used: $t = r\sqrt{\frac{n-2}{1-r^2}}$. Note that the Pearson correlation test treats the BNT score as a continuous variable, implying that the intelligence difference between the score levels is about equal.}
As expected the correlation is negative, which implies that more intelligent individuals tended to be more accurate forecasters (expressed in a lower Brier score). This supports the first hypothesis (1a) in line with prior findings in the literature. However, the relationship is neither strong nor clearly significant. There can be several reasons for this: First, the relation between intelligence and forecasting accuracy might be less strong than previously argued. This is, however, rather unlikely, as intelligence was a strong predictor of forecasting success in more sophisticated research designs [@Mellers.2015; @Poore.2014]. Second, the BNT test could be not valid and therefore not measure what it claims to. Again, this is rather unlikely as it has been extensively tested in various settings [@Cokely.2012]. Nevertheless, one might argue that it measures an intelligence dimension which is less relevant for forecasting than other intelligence aspects. However, BNT score are correlated with other intelligence measures [@Cokely.2012] and there is no plausible reason why risk literacy should not matter for forecasting while other intelligence measures do. Third and most likely: The forecasting success in the tournament reflects a mix of skills and luck, which is skewed towards luck. This was already indicated by the rather moderate performance of the crowd against standard benchmarks. The reason is probably the one-off nature of the forecasting tournament. In contrast to the forecasting projects of @Mellers.2015 and @Poore.2014 participants have little chance to incorporate feedback and improve their forecasting skills under these conditions. If this holds true, it is likely to be reflected as well in the remaining hypotheses testing.

<!-- Alternative: *b. The correlation is not as expected as a better BNT score is not related to more forecasting accuracy. Hence, we cannot confirm the first hypothesis (1a). There are several possible reasons for this. First, the hypothesed relation could be wrong. This is, however, rather unlikely, as it has been shown before in more sophisticated research designs [@Mellers.2015; @Poore.2014]. Second, the test could be not valid and therefore not measure what it claims  to. Again, this is rather unlikely as it has been extensively tested [@Cokely.2012] but it cannot fully be ruled out that the risk literacy it measures is not relevant for forecasting. Third and most likely: The forecasting success in this tournament rather a result of luck than skill. Since this was a one-off event where the forecasters had little chances of learning, forecasting skill might not have cristalized in the results and might only be accessable in a more long-term setting. However, it does also imply that the results for the other hypotheses should also be read with care as chance might be the primary driver behind the forecasting scores.* -->

<!-- alternative comments on significance  *a. The significance of the finding further underlines the point. b. But due to the lack of significance, not final conclusion can be made.* -->

```{r results="asis", echo = F}
cor.plot
```

Having this in mind, I can now turn to the second dispositional factor under scrutiny: moral competency. The Moral Competency Test (MCT) resulted in scores ranging from `r round(min(SPFT$mct.c, na.rm = TRUE),2)` to `r round(max(SPFT$mct.c, na.rm = TRUE),2)`. Similar ranges were also found in other groups [@Lind.2008].<!-- change, if outlier continues to exist, to: Except for a few high outliers (>0.5) this reflects the pattern found in other groups. --> The moral competency score is not available for all participants since the completion of the moral competency test questions were not obligatory. However, the missing values are largely due to forgeting to answer a sub-question than to categorical non-replies. ^[There is no procedure for calculating the MCT score with missing answers.] Hence, the scores should not have systematic non-resonse bias. In order to assess the hypothesis, the Pearson coefficient is used to evaluate the relationship between forecasting accuracy and moral competency: `r cor.brier.mct`.

```{r echo=FALSE, fig.cap="Scatterplot Moral Competency and Brier Score", out.width=c('300px', '150px'), fig.align='center', fig.pos = 'H'}
cor.brier.mct.plot
```

<!-- evaluation hypothesis 1b --> 
The correlation between moral competency and forecasting accuracy indicates that individuals with a higher moral competency score forecasted slightly more accurately. However, the effect is very weak and clearly not significant. Hence, it is not possible to draw a solid conclusion on the relationship between moral competency and forecasting accuracy. Like for the intelligence hypothesis, the role of luck versus skill might skew the result. However, in the case of moral competency one other major reason might also play a role: The validity of the MCT test for the hypothesis. The test is a valid measure to see whether people consistently evaluate moral questions on the basis of their moral quality when faced with difficult decisions [@Lind.2008, p. 200]. In other words: It looks whether individuals are considering different moral reasons for actions despite having a moral position for themselves. In the context of the forecasting tournament the questions whether the moral opinion of the forecasters affects their analytic judgements. This is not exactly the same. Nevertheless, there is a similarity in both tasks: In both cases the decision makers have to consider contrary views and incorporate them into their judgement. Hence, they have to be willing and able to incorporate new information while having a personal stand on the issue.<!-- This is similar to @Baron.2007 concept of active open mindedness, which has been a good predictor for forecasting success [@Mellers.2015].--> The link between moral and analytical judgements can, however, only be disentangled with further research.

<!-- Alternatives
*a. The correlation between both indicates that moral competency is a good predictor for forecasting accuracy. This supports our Hypothesis 1b. [The result is even significant / However, the result is not significant.].*
*b. The correlation shows hardly any correlation between moral competency and forecasting accuracy score. Hence, the hypothesis is not supported by the data. There are different reason for this. First, there might be no connection between moral competency and forecasting skills. One might argue moral judgements and analytical decisions are different things, which have no relationship to each other. However, the moral competency measure explicitly includes the cognitive elements of moral thinking. [@Lind.2008] Second, moral competency might not measure the ability to separate moral from analytical judgements. This is a valid criticism, as the measure was developed in a different context and might not smoothly transfer to the discussion of forecasting skills. Third, the forecasting success  of the participants might be grounded rather in luck than their ability to separate moral from analytical judgements.*
-->

<!-- Descripitves  Hypothesis 2 -->
In hypothesis 2 the role of decision time for forecasting accuracy was tested. The median participants spend `r round(median(SPFT$Duration.min),1)` minutes for participating in the survey and `r round(median(SPFT$time.fq.sec, na.rm = TRUE),1)` minuted on answering the forecasting questions. Hence, a large share of the participants did not spend much time on the individual questions but relied on their intuitive judgement.^[Overall, `r round(intu.share,1)`% of the participants said they used only or mostly intuition to answer the questions.] But this is not true for all participants, as some spend a considerable amount of time on forecasting. Hence, there is sufficient variance among the participants for the hypothesis. Since hypothesis 2 assumes the marginal benefit of time to decrease, the logarithmized time for answering the forecasting question is used instead of the actual time. The Pearson correlation between the logarithmized time and forecasting accuracy is: `r cor.brier.time`.^[Two extreme outliers were excluded as the recorded time was unplausibly high, likely as a result of interrupting the survey in order to do something else.] 
<!-- Discussion of second method to test the relevance of time: Divide participants into two groups depending on the time they spend in the questions and compare group means. The exact devision time is however arbitrary.-->

<!-- evaluation of Hypothesis 2 --> 
The negative correlation coefficient indicates that more time spend on answering the question is related to more forecasting accuracy. Moreover, when we compare the correlation of the linear time and the logarithmized time, the later has a higher correlation  coefficient and clearly  significant. (`r round(cor(SPFT$brier.avg, SPFT$time.fq.sec.log, use="complete.obs"),2)` > `r cor.brier.time.linear`). Hence, this supports the view of a decreasing marginal return on forecasting as it was expected in hypothesis 2. To conclude: Forecasters who used more time performed better, but the added accuarcy decreased as more time they spend on the forecasting questions. 

<!-- ALTERNATIVE However, the relationship is not significant]
*The positive correlation is contrary to the time hypothesis. This might [again] be an indicator that forecasting success in this forecasting tournament is rather due to luck than to other factors. It can also mean that other aspects, e.g. prior political knowledge, offset the effects of time for forecasting. This could only be investigated in further research.* -->

```{r echo = FALSE, fig.pos='H', fig.cap= "Forecasting time - Brier score scatterplot", out.width=c('240px', '120px'), fig.align='center'}
cor.brier.time.plot
```

```{r echo = FALSE, fig.pos='H', fig.cap= "Log (forecasting time) - Brier score scatterplot", out.width=c('240px', '120px'), fig.align='center'}
cor.brier.time.log.plot
```
<!-- Hypothesis 3 -->
Finally, hypothesis 3 was about the effect of an decision aid intervention. The participants were randomly assigned to a treatment (n = `r plyr::count(SPFT$Group[SPFT$Group == "Treatment"])[2]`) or control group (n = `r plyr::count(SPFT$Group[SPFT$Group == "Control"])[2]`). The treatment group was presented with a decision guide, while the control group was not. To test whether the analytic guide had any impact on the forecasting accuracy the mean Brier scores of both groups is used. The mean Brier score of the treatment group is `r round(t.test.intervention[[5]][1],2)` and of the control group `r round(t.test.intervention[[5]][2],2)`. 

<!-- evaluation hyptothesis 3 -->
There is hardly any difference between the two groups (`r t.test.intervention.result`). Hence, it is unlikely that the intervention had any impact on forecasting accuracy and there is no support for the intervention hypothesis (3).
There are several possible reasons for the failure of the decision guide: 

<!-- first reason: ignored decision guide -->
First, participants might have ignored the decision guide. This might in particular be true for forecasters who only spend a few minutes on answering the questions. Moreover, applying a decision methodology requires cognitive effort from decision makers [@Kretz.2015, p. 68ff] and disrupts the train of thoughts [@Hernandez.2013]. When faced with difficult analytical tasks, like forecasting questions, decision makers might have used their mental capacities rather for information processing than for their methodological approach. 
The intervention tried to account for this with a minimal preventative measure. The forecasters had to indicate with a check box whether they have read the the guide. Check boxes have been shown to be an effective nudge for analysts to increase their attention and are commonly used, e.g. by airline pilots or marine crews, see @Kretz.2015 [p. 33ff.]. All participants in the treatment group indicated that they have read the guide. 
So what happened? To see whether the treatment lead to any behavioural change, we can check whether it had any effect on the time used for answering the forecasting questions. And this is the case, as the treatment group used `r round(mean(SPFT$time.fq.sec[SPFT$Group == "Treatment"], na.rm = TRUE),1)` min and the control group `r round(mean(SPFT$time.fq.sec[SPFT$Group == "Control"], na.rm = TRUE),1)` min. But these means might be the result of few extreme outliers. To compare whether the difference is more than a random occurance, the logarithmized times can be used.^[Eye-balling the data shows that time in minutes used by the forecasters clearly does not follow a Gaussian distribution, but the logarithmized distribution ressembles the classical Gaussian curve.] Applying a t-test shows that the treatment group used more time if we use a significance level of 10%, but not for 5% (`r t.test.intervention.time`). Even though this is not a clear result, it does also not rule out a small behavioural change from the decision guide.

```{r echo = FALSE, fig.pos='H', fig.cap= "Distribution forecasting log(time) for treatment and control group ", out.width=c('240px', '120px'), fig.align='center'}
hypo3.time.plot
```
<!-- check the written feedback -->

But even if the forecasters used the decision methodology, the lack of higher accuracy might also come from irrelevancy of the proposed methodology. However, this would contradict the research results by @Tetlock.2015 and his so-called superforecasters, who performed rather well with the methodology of the decision guide in the Good Judgement Project. Nevertheless, the problem might by a result of inappropiate application of the decision heuristic. In other words: In order to have measurable results in terms of forecasting accuracy, the forecasters need succicient training in the methodology and a simple decision guide does just not provided enough learning experience. This is the most plausible explanation for the intervention failure, as for example @Mellers.2014 showed how a one-hour probabilistic training, which included the inside outside view methodology, could improve forecasting accuracy on the long term.

<!-- general conclusion --> 
To conclude, similar to the 'list the hypotheses' or 'map the evidence methodology' [@Kretz.2015] a simple decision guide can be added to a list of decision aids with no to limited effects for analytical decisions. However, what exactly is the reason for the failure is less clear.

<!-- test whether the treatment had any effect on time as a validity / verification measure  whether people actually used the heuristic -->

<!-- Alternative: *a. The differences between die groups indicates that the intervention had an impact on forecasting accuracy in line with hypothesis 3. [The difference is significant, which is * -->

<!-- refining the results: looking only at forecasters who spend sufficient time on answerin the questions
First test: Improves result for hypothesis 1a slightly and worsens hypothesis 1b -->

<!-- using a structural equation model to estimate the relationship between single factors -->


# Aggregation of crowd forecasts

<!-- agregating forecasts
1. extremizing -> 80% to 95%, 20% to 5% 
2. weighting responses by intelligence and time invested
-->

Initially, we defined the wisdom of the crowd with the mean forecast of the forecasting competition participants. But if we take the idea of individual differences in forecasting skill seriously, the crowd wisdom can be aggregated in a more effective way by taking into account what we know about individual forecasting decisions. On the one hand, we can use individual predictors of forecasting success to weight the individual judgements appropiately. On the other hand, the individual forecasts can be recalibrated based on commonly exhibited decision making biases.

<!-- different available methods of aggregating-->
1. standard approach: averaging
2. weighted average:
    also called linear opinion polling
    e.g. weights are assigned by imporante of expert
    but it has been shown that linear combinations don't perform optimally (Ranjan and Gneiting, 2010; Allard, Comunian, Renard 2012)
3. extremized weighted averages [@Satopaa.2015b]
    a. @Satopaa.2014 use a simple logit model
    b. @Satopaa.2017 further use information about diversity of information sources

<!-- weightening / dropping forecasts  -->
Adjusting the forecasts with most promising predictors of forecasting success: Intelligence and time spend on forecasting.

First, drop all forecasts with the lowest BNT score. These are participants who did not get a single question right in the BNT test. Second, use the remaining scores to weight the forecasts. Chossing the exact weights is slightly arbitrary.

Similar proceedure: Drop one-quarter of forecasts with the lowest time spend on answering the questions. The exact share of dropped respondents is also arbitrary. The remaining respondents are weighted according to their logarithmized time. Using the logaritmized time instead of the actual time is consistent with the finding on hypothesis 2 and reduces the impact of outliers on the aggregated forecast.

Other predictors available in the forecasting tournament data have not been supported sufficiently (moral competency, intervention). 

<!-- debiasing / extremizing -->

humans have problems with very low and very high probabilities. This problem is for example visible in the long-shot bias in prediction markets. It has also lead to a longstanding debate in behavioural economics (). 

this knowledge can be used to fine-tune forecasting aggregation. The basic idea is to extremize individual responses if they are in the range of problematic forecasts. If forecasters assign a low probability score to events, they are assumed to still overestimate the likelihood of the event. Likewise, if forecasters choose a high probability for an event they are assumed to underestimate the actual probability and there is a reason to adjust their judgment to a higher probability value. More difficult, however, is decide how strongly probability judgements should be adjusted and what exactly the range of "problematic" probability is.

Again, there are also other other biases one could correct for. For example, forecasting questions are subject are to a reference bias (Kahnemann/Tetlock?). This bias claims that forecasters make likelihood judgements independently of the time horizon under consideration. Hence, forecasters would assign the same probability to an event happening in the next 3 months as to the same event in the next 6 months if they are only presented with one of these questions. However, it is difficult to correct for this bias ex-post. Extremizing on contrast has already successfully been used to calibrate crowd aggregation and for the sake of simplicity only one bias will be corrected here.

<!-- results of smart crowd aggregation -->


# Discussion
<!--	General findings -->
To sum up, this reasearch has found the forecasting crowd to perform slightly better than random quessing but not as accurate as in other comparable forecasting competitions. The data provides indicative support that intelligence and moral competency are related to more forecasting accuracy, but findings are to weak to make a final conclusion on them. The forecasting tournament strongly supports the idea that more forecasting time leads to more forecasting accuracy, but with diminishing returns. Finally, there is no support that the mild decision guide used as intervention has any measurable effect on forecasting accuracy. 

<!--	Limitations of the design -->
- only a one-off event

- participant number relatively small compared to other forecasting competitions in the field

- intervention limited due to online format

<!--	Policy relevance 
[on moral competency]
Other than prior mentioned dispositional factors this is and should not be a factor for selecting forecasters. One way to see it would be the institutional level: Moral judgements are constantly made by politicians and they are essential in the political discourse. If moral judgements however inflict with forecasting, there is a reason to separate forecasting from actual decision making. This could be either than by tasking different people with this and/or separate it institutionally. But the morality hypothesis could only be one indicator for this. 
http://blog.oup.com/2015/08/policy-makers-moral-dilemmas/ -->

# References

