---
title: "Security Policy Forecasting Tournament"
subtitle: "Frequently Asked Questions"
author: ""
output: html_document
---

## Introduction

## Brier Score

The Brier score was originally proposed to quantify the accuracy of weather forecasts, but can be used to describe the accuracy of any probabilistic forecast. Roughly, the Brier score indicates how far away from the truth your forecast was.

The Brier score is the squared error of a probabilistic forecast. To calculate it, we divide your forecast by 100 so that your probabilities range between 0 (0%) and 1 (100%). Then, we code reality as either 0 (if the event did not happen) or 1 (if the event did happen). For each answer option, we take the difference between your forecast and the correct answer, square the differences, and add them all together. For a yes/no question where you forecasted 70% and the event happened, your score would be (1 – 0.7)2 + (0 – 0.3)2 = 0.18. The best (lowest) possible Brier score is 0, and the worst (highest) possible Brier score is 2.

For further information see [Wikipedia](https://en.wikipedia.org/wiki/Brier_score) or [The Good Judgement Open](https://www.gjopen.com/faq).
