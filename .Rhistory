rand(5,5)
ran(5,5)
rnorm(1)
replicate(10, rnorm(1))
replicate(10, rnorm(1))
replicate(10, rnorm(2))
matrix(rnorm(12),4,3)
(1:10)/10
(1:10)*2+3
tvector <- (1:20)*2-4
tvector <- tvector+replicate(20, rnorm(1))
clear
clear()
rm(list = ls(all = TRUE))
tvector <- (1:20)*2-4
tvector <- tvector+replicate(20, rnorm(1))
x1 <- (1:20)
lm
lm(tvector ~ x1)
rm(list = ls(all = TRUE))
rm(list = ls())
library(rvest)
library(dplyr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce <- url %>% read_html() %>%
html_nodes('.pane-content') %>%
html_text()
osce
view(osce)
print(osce)
rm(list = ls())
library(rvest)
library(dplyr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce <- url %>% read_html() %>%
html_nodes('.pane-content') %>%
html_text() %>% trim
rm(list = ls())
library(rvest)
library(dplyr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce <- url %>% read_html() %>%
html_nodes('.pane-content') %>%
html_text() %>% trim = T
rm(list = ls())
library(rvest)
library(dplyr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce <- url %>% read_html() %>%
html_nodes('.pane-content') %>%
html_text(trim = T)
print(osce)
rm(list = ls())
library(rvest)
library(dplyr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce <- url %>% read_html() %>%
html_nodes('.pane-node-body') %>%
html_text(trim = T)
print(osce)
rm(list = ls())
library(rvest)
library(dplyr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce.heading <- url %>% read_html() %>%
html_nodes('.content--header') %>%
html_text(trim = T)
osce.content <- url %>% read_html() %>%
html_nodes('.pane-node-body') %>%
html_text(trim = T)
print(osce.heading)
help(gsub)
osce.heading %>% gsub("\n", "")
osce.heading %>% gsub("\n", "")
gsub("\n", "", osce.heading)
install.packages("stringr")
rm(list = ls())
library(rvest)
library(dplyr)
library(stringr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce.heading <- url %>% read_html() %>%
html_nodes('.content--header') %>%
html_text(trim = T)
trash <- c("Facebook", "Twitter", "LinkedIn", "Google+", "Share", "English", "Русский", "Українська")
osce.heading <- gsub("\n", "", osce.heading)
osce.heading <- gsub(trash, "", osce.heading)
osce.content <- url %>% read_html() %>%
html_nodes('.pane-node-body') %>%
html_text(trim = T)
osce.heading <- gsub("[ - ] Latest from", "Latest from", osce.heading)
help("str_trim")
osce.heading <- gsub("[ - ]", "Latest from", osce.heading)
osce.heading <- gsub("[ - ]", "Latest from", osce.heading)
osce.heading <- gsub("[ - ]", "Latest from", osce.heading)
osce.heading <- gsub("[ - ]", "Latest from", osce.heading)
osce.heading <- gsub("[ - ]", "Latest from", osce.heading)
osce.heading <- gsub("[ - ]", "Latest from", osce.heading)
osce.heading <- sub("[ - ]", "Latest from", osce.heading)
osce.heading <- sub("[ - ]", "", osce.heading)
osce.content <- url %>% read_html() %>%
html_nodes('.pane-node-body') %>%
html_text(trim = T)
osce.heading <- sub("[ - ]", "", osce.heading)
osce.heading <- sub("[ - ]", "", osce.heading)
rm(list = ls())
library(rvest)
library(dplyr)
library(stringr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce.heading <- url %>% read_html() %>%
html_nodes('.content--header') %>%
html_text(trim = T)
trash <- c("Facebook", "Twitter", "LinkedIn", "Google+", "Share", "English", "Русский", "Українська")
osce.heading <- gsub("\n", "", osce.heading)
osce.heading <- sub("*Latest", "Latest", osce.heading)
osce.content <- url %>% read_html() %>%
html_nodes('.pane-node-body') %>%
html_text(trim = T)
osce.heading <- sub("*Latest", "Latest", osce.heading)
osce.heading <- gsub("*Latest", "Latest", osce.heading)
osce.heading <- gsub("^.*?Latest","Latest", osce.heading)
print heading
print(osce.heading)
osce.content <- gsub("\n", "", osce.content)
print(osce.content)
rm(list = ls())
rm(list = ls())
library(rvest)
library(dplyr)
library(stringr)
url <- "http://www.osce.org/ukraine-smm/276006"
# Get and parse expenses_table from the webpage
osce.heading <- url %>% read_html() %>%
html_nodes('.content--header') %>%
html_text(trim = T)
osce.heading <- gsub("\n", "", osce.heading)
osce.heading <- gsub("^.*?Latest","Latest", osce.heading)
osce.content <- url %>% read_html() %>%
html_nodes('.pane-node-body') %>%
html_text(trim = T)
osce.content <- gsub("\n", "", osce.content)
print(osce.content)
print(osce.heading)
install.packages("googleVis")
library(googleVis)
demo(googleVis)
library(googleVis)
require(datasets)
states <- data.frame(state.name, state.x77)
GeoStates <- gvisGeoChart(states, "state.name", "Illiteracy",
options=list(region="US",
displayMode="regions",
resolution="provinces",
width=600, height=400))
plot(GeoStates)
help("gvisGeoChart")
library(googleVis)
require(datasets)
states <- data.frame(state.name, state.x77)
GeoStates <- gvisGeoChart(states, "state.name", "Illiteracy",
options=list(region="DE",
displayMode="regions",
resolution="provinces",
width=600, height=400))
plot(GeoStates)
install.packages("devtools")
##################################################
## File for extracting data, cleaning and calculating scores
## by: Alexander Sacharow
##################################################
##################################################
# CONTENT
# 0. Preparations
# 1. Get Data
# 2. MTurk
# 3.
##################################################
##################################################
# 0. Preparations
##################################################
# Clear Global environment
rm(list=ls())
## Setting Working directory
try(setwd("D:/Eigene Datein/Dokumente/Uni/Hertie/Materials/Master thesis/SecurityPolicyForecastingTournament"), silent = TRUE)
# Collect packages/libraries we need:
packages <- c("readxl", "dplyr")
# package and why it is needed
# readxl: import excel files
# dyplyr: data manipulation
# install packages if not installed before
for (p in packages) {
if (p %in% installed.packages()[,1]) {
require(p, character.only=T)
}
else {
install.packages(p, repos="http://cran.rstudio.com", dependencies = TRUE)
require(p, character.only=T)
}
}
rm(p, packages)
################################################
# 1. Import Data
################################################
# Import responses
SPFT.Main <- read.csv2("raw/SPFT-20170217.csv", sep = ",")
SPFT.MTurk <- read.csv2("raw/SPFT_MTurk_20170210.csv", sep = ",")
# Import realized outcomes
FQ <- read_excel("raw/SPFT-questions-test.xlsx")
# import MTurk batch results
MTurk.batch <- read.csv2("raw/Batch_2677782_batch_results.csv", sep = ",")
MTurk.batch <- MTurk.batch %>% select(WorkerId, WorkTimeInSeconds, Answer.surveycode, Approve, Reject)
###############################################
# 2. Clean Data
###############################################
# Merge SPFT and SPFT Turk
SPFT <- dplyr::bind_rows(SPFT.Main, SPFT.MTurk[-(1:2),], .id = "OriginRespond")
# Remove unfinished surveys
SPFT <- SPFT %>% dplyr::filter(Finished == "True")
# Change variable types ########################
# Dates
SPFT$StartDate <- as.Date(as.character(SPFT$StartDate))
SPFT$EndDate <- as.Date(as.character(SPFT$EndDate))
SPFT$Duration..in.seconds. <- as.numeric(SPFT$Duration..in.seconds.)
# Remove test surveys
SPFT <- SPFT %>% filter(StartDate > "2017-02-05")
# Drop empthy forecasts (Mainly MTurk failed attention check)
SPFT <- SPFT %>% filter(time.sec2_First.Click != "")
# delete unnecessary information
SPFT <- SPFT %>% select(-Status, -Progress, -Finished,
-RecipientLastName, -RecipientFirstName,
-RecipientEmail, -LocationLatitude, -LocationLongitude,
-DistributionChannel, -StartDate, -ExternalReference,
-further.info...Topics, -contains("Click") )
# delete raw files
rm(SPFT.Main, SPFT.MTurk)
####################################################
# 3. Identify problematic MTurk users
###################################################
# only MTurk
MTurk <- SPFT %>% filter(id.mturk != "")
# Criterion 1: Check failed attention checks
MTurk$c1 <- 0
MTurk[MTurk$att.check != 48,]$c1 <- 1
# Criterion 2: All BNT questions wrong
MTurk <- MTurk %>%
mutate(c2 = ifelse(bnt1 != 30 & bnt2 != 25 & bnt3 != 20 & bnt4 != 50,
1, 0))
# Criterion 3: Short time
# summary(MTurk$Duration..in.seconds.)
MTurk <- MTurk %>% mutate(c3 = ifelse(Duration..in.seconds. < 600, 1, 0))
# Criterion 4: Doublication in unique identifiers (IP, email, MTurk ID)
MTurk$c4.ip <- 0
MTurk$c4.email <- 0
email.double <- data.frame(table(MTurk$personal.report))
# email.double[email.double$Freq > 1,]
MTurk[MTurk$personal.report %in% email.double$Var1[email.double$Freq > 1 & email.double$Freq < 10],][,"c4.email"] <- 1
IP.double <- data.frame(table(MTurk$IPAddress))
# IP.double[IP.double$Freq > 1,]
MTurk[MTurk$IPAddress %in% IP.double$Var1[IP.double$Freq > 1],][,"c4.ip"] <- 1
# merge dataframe by completion code
MTurk <- merge(MTurk, MTurk.batch, by.x = "CompletionCode", by.y = "Answer.surveycode", all = TRUE)
# check WorkerID code
MTurk$WorkerId <- trimws(as.character(MTurk$WorkerId))
MTurk$id.mturk <- trimws(as.character(MTurk$id.mturk))
MTurk$compWorkID <- ifelse(MTurk$WorkerId == MTurk$id.mturk, 1, 0)
# compute total control fails
MTurk$c.total <- MTurk$c1 + MTurk$c2 + MTurk$c3 + MTurk$c4.ip + MTurk$c4.email
MTurk.Prob <- MTurk %>% select(ResponseId, id.mturk, CompletionCode, c1, c2, c3, c4.ip, c4.email, c.total)
MTurk.batch <- merge(MTurk.batch, MTurk.Prob, by.x = "Answer.surveycode", by.y = "CompletionCode")
# Reject if attention check fail and (indication for double user or 3 total control fails)
MTurk.batch[MTurk.batch$c1 == 1 & MTurk.batch$c4.email == 1, "Reject"] <- TRUE
MTurk.batch[MTurk.batch$c1 == 1 & MTurk.batch$c4.ip == 1, "Reject"] <- TRUE
MTurk.batch[MTurk.batch$c1 == 1 & MTurk.batch$c.total > 2, "Reject"] <- TRUE
# View rejected Mturk users
View(MTurk.batch %>% filter(Reject == T))
################################################
# 4. Score board
################################################
# recode FQ to binary
FQ[,4] <- 0
FQ[FQ[,3] == "yes", 4] <- 1
colnames(FQ) <- c(colnames(FQ)[1:3], "out")
# score board data (MTurk resondents excluded)
SB <- SPFT  %>% filter(OriginRespond != 2) %>%
select(ResponseId, id.hertie, id.other, id.mturk, starts_with("fq"))
SB <- SB[-(1:2),]
# transform responses to percentages
SB[,-(1:4)] <- sapply(sapply(SB[,-(1:4)],as.character),as.numeric)
SB[,-(1:4)] <- SB[,-(1:4)]/100
## calculate brier scores for each question/respondent
# number of questions
q.num <- 24
#i <- 1
for(i in 1:q.num){
tmp <- paste("fq", i, sep = "")
# add outcome in new brier score column (from question xlsx)
SB[,paste(tmp,"bs", sep = ".")] <- as.numeric(FQ[FQ[,1] == tmp, 4])
# compute difference outcome and quess
SB[,paste(tmp,"tmp1", sep = ".")] <- select(SB, i+4+q.num) - select(SB, i+4)
# compute difference outcome  and counterfactual
SB[,paste(tmp,"tmp2", sep = ".")] <- (1 - select(SB, i+4+q.num)) - (1- select(SB, i+4))
# Square differences and sum them
SB[,paste(tmp,"bs", sep = ".")] <- SB[,paste(tmp,"tmp1", sep = ".")]*SB[,paste(tmp,"tmp1", sep = ".")] +
SB[,paste(tmp,"tmp2", sep = ".")]* SB[,paste(tmp,"tmp2", sep = ".")]
# delete unneccessary columns
SB <- SB %>% select(-contains("tmp"))
rm(tmp)
}
# Compute average brier score for each respondent
SB[,"brier.avg"] <- rowMeans(select(SB, contains("bs")))
# Sort by brier score
SB <- SB %>% arrange(brier.avg)
View(SPFT)
View(FQ)
View(FQ)
##################################################
## File for extracting data, cleaning and calculating scores
## by: Alexander Sacharow
##################################################
###############################################################################
# CONTENT
# 0. Preparations
# 1. Get Data
# 2. Clean data
# 3. MTurk
# 4. Forecast summary
# 5. Scoreboard
###############################################################################
###############################################################################
# ToDo
# 1. Exclude Mturk rejects from sample
# 2. Restrict sample to Feb. 12
# 3. Export MTurk script
# 4. Hertie Response summary
# 5. Compute MCT and BNT scores
# 6. Upload aggregates forecasting responses
#    a. Compute group means Hertie, Volunteers, MTurks
###############################################################################
###############################################################################
# 0. Preparations
###############################################################################
# Clear Global environment
rm(list=ls())
## Setting Working directory
try(setwd("D:/Eigene Datein/Dokumente/Uni/Hertie/Materials/Master thesis/SecurityPolicyForecastingTournament"), silent = TRUE)
# Collect packages/libraries we need:
packages <- c("readxl", "dplyr")
# package and why it is needed
# readxl: import excel files
# dyplyr: data manipulation
# install packages if not installed before
for (p in packages) {
if (p %in% installed.packages()[,1]) {
require(p, character.only=T)
}
else {
install.packages(p, repos="http://cran.rstudio.com", dependencies = TRUE)
require(p, character.only=T)
}
}
rm(p, packages)
###############################################################################
# 1. Import Data
###############################################################################
# Import responses
SPFT.Main <- read.csv2("raw/SPFT-20170217.csv", sep = ",")
SPFT.MTurk <- read.csv2("raw/SPFT_MTurk_20170210.csv", sep = ",")
# Import realized outcomes
FQ <- read_excel("raw/SPFT-questions-test.xlsx")
# import MTurk batch results
MTurk.batch <- read.csv2("raw/Batch_2677782_batch_results.csv", sep = ",")
MTurk.batch <- MTurk.batch %>% select(WorkerId, WorkTimeInSeconds, Answer.surveycode, Approve, Reject)
###############################################################################
# 2. Clean Data
###############################################################################
# Merge SPFT and SPFT Turk
SPFT <- dplyr::bind_rows(SPFT.Main, SPFT.MTurk[-(1:2),], .id = "OriginRespond")
# Remove unfinished surveys
SPFT <- SPFT %>% dplyr::filter(Finished == "True")
# Change variable types ########################
# Dates
SPFT$StartDate <- as.Date(as.character(SPFT$StartDate))
SPFT$EndDate <- as.Date(as.character(SPFT$EndDate))
SPFT$Duration..in.seconds. <- as.numeric(SPFT$Duration..in.seconds.)
# Remove test surveys
SPFT <- SPFT %>% filter(StartDate > "2017-02-05")
# Drop empthy forecasts (Mainly MTurk failed attention check)
SPFT <- SPFT %>% filter(time.sec2_First.Click != "")
# delete unnecessary information
SPFT <- SPFT %>% select(-Status, -Progress, -Finished,
-RecipientLastName, -RecipientFirstName,
-RecipientEmail, -LocationLatitude, -LocationLongitude,
-DistributionChannel, -StartDate, -ExternalReference,
-further.info...Topics, -contains("Click") )
# delete raw files
rm(SPFT.Main, SPFT.MTurk)
###############################################################################
# 3. Identify problematic MTurk users
###############################################################################
# only MTurk
MTurk <- SPFT %>% filter(id.mturk != "")
# Criterion 1: Check failed attention checks
MTurk$c1 <- 0
MTurk[MTurk$att.check != 48,]$c1 <- 1
# Criterion 2: All BNT questions wrong
MTurk <- MTurk %>%
mutate(c2 = ifelse(bnt1 != 30 & bnt2 != 25 & bnt3 != 20 & bnt4 != 50,
1, 0))
# Criterion 3: Short time
# summary(MTurk$Duration..in.seconds.)
MTurk <- MTurk %>% mutate(c3 = ifelse(Duration..in.seconds. < 600, 1, 0))
# Criterion 4: Doublication in unique identifiers (IP, email, MTurk ID)
MTurk$c4.ip <- 0
MTurk$c4.email <- 0
email.double <- data.frame(table(MTurk$personal.report))
# email.double[email.double$Freq > 1,]
MTurk[MTurk$personal.report %in%
email.double$Var1[email.double$Freq > 1 & email.double$Freq < 10],][,"c4.email"] <- 1
